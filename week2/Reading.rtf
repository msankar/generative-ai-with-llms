{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red24\green24\blue24;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c12157\c12157\c12157;\cssrgb\c100000\c100000\c100000;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa199\partightenfactor0

\f0\b\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Multi-task, instruction fine-tuning
\f1\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2210.11416.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 Scaling Instruction-Finetuned Language Models
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - Scaling fine-tuning with a focus on task, model size and chain-of-thought data.\cb1 \uc0\u8232 \
\ls1\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - This blog (and article) explores instruction fine-tuning, which aims to make language models better at performing NLP tasks with zero-shot inference.\cb1 \uc0\u8232 \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 Model Evaluation Metrics
\f1\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://crfm.stanford.edu/helm/latest/"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 HELM - Holistic Evaluation of Language Models
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - HELM is a living benchmark to evaluate Language Models more transparently. \cb1 \uc0\u8232 \
\ls2\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://openreview.net/pdf?id=rJ4km2R5t7"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 General Language Understanding Evaluation (GLUE) benchmark
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \'a0- This paper introduces GLUE, a benchmark for evaluating models on diverse natural language understanding (NLU) tasks and emphasizing the importance of improved general NLU systems.\cb1 \uc0\u8232 \
\ls2\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://super.gluebenchmark.com/"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 SuperGLUE
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - This paper introduces SuperGLUE, a benchmark designed to evaluate the performance of various NLP models on a range of challenging language understanding tasks.\cb1 \uc0\u8232 \
\ls2\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://aclanthology.org/W04-1013.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 ROUGE: A Package for Automatic Evaluation of Summaries
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - This paper introduces and evaluates four different measures (ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S) in the ROUGE summarization evaluation package, which assess the quality of summaries by comparing them to ideal human-generated summaries.\cb1 \uc0\u8232 \
\ls2\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2009.03300.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 Measuring Massive Multitask Language Understanding (MMLU)
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - This paper presents a new test to measure multitask accuracy in text models, highlighting the need for substantial improvements in achieving expert-level accuracy and addressing lopsided performance and low accuracy on socially important subjects.\cb1 \uc0\u8232 \
\ls2\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2206.04615.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - The paper introduces BIG-bench, a benchmark for evaluating language models on challenging tasks, providing insights on scale, calibration, and social bias.\cb1 \uc0\u8232 \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 Parameter- efficient fine tuning (PEFT)
\f1\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2303.15647.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - This paper provides a systematic overview of Parameter-Efficient Fine-tuning (PEFT) Methods in all three categories discussed in the lecture videos.\cb1 \uc0\u8232 \
\ls3\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2211.15583.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 On the Effectiveness of Parameter-Efficient Fine-Tuning
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - The paper analyzes sparse fine-tuning methods for pre-trained models in NLP.\cb1 \uc0\u8232 \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 LoRA
\f1\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2106.09685.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 LoRA Low-Rank Adaptation of Large Language Models
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  
\f1\b0 -  This paper proposes a parameter-efficient fine-tuning method that makes use of low-rank decomposition matrices to reduce the number of trainable parameters needed for fine-tuning language models.\cb1 \uc0\u8232 \
\ls4\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2305.14314.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 QLoRA: Efficient Finetuning of Quantized LLMs
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - This paper introduces an efficient method for fine-tuning large language models on a single GPU, based on quantization, achieving impressive results on benchmark tests.\cb1 \uc0\u8232 \
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 \cb3 Prompt tuning with soft prompts
\f1\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0
\f0\b \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://arxiv.org/pdf/2104.08691.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul \outl0\strokewidth0 \strokec2 The Power of Scale for Parameter-Efficient Prompt Tuning
\f1\b0 \cb1 \ulnone \uc0\u8232 \u8232 }}
\f1\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2  - The paper explores "prompt tuning," a method for conditioning language models with learned soft prompts, achieving competitive performance compared to full fine-tuning and enabling model reuse for many tasks.\cb1 \uc0\u8232 \
}